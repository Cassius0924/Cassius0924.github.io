<!doctype html><html lang=zh dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>LLM 参数之 Response Format | Cassius0924 的博客</title><meta name=keywords content="LLM,AI,JSON"><meta name=description content="深入探讨 LLM 的 Response Format 参数，如何确保生成内容符合预期格式。"><meta name=author content><link rel=canonical href=https://blog.cassdev.com/posts/llm-param-response-format/><link crossorigin=anonymous href=/assets/css/stylesheet.4f5249ac184c033613d94c5267209e90161d3a93365c2feb0ad3960a4bbb6393.css integrity="sha256-T1JJrBhMAzYT2UxSZyCekBYdOpM2XC/rCtOWCku7Y5M=" rel="preload stylesheet" as=style><link rel=icon href=https://blog.cassdev.com/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://blog.cassdev.com/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://blog.cassdev.com/favicon-32x32.png><link rel=apple-touch-icon href=https://blog.cassdev.com/apple-touch-icon.png><link rel=mask-icon href=https://blog.cassdev.com/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=zh href=https://blog.cassdev.com/posts/llm-param-response-format/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://blog.cassdev.com/posts/llm-param-response-format/"><meta property="og:site_name" content="Cassius0924 的博客"><meta property="og:title" content="LLM 参数之 Response Format"><meta property="og:description" content="深入探讨 LLM 的 Response Format 参数，如何确保生成内容符合预期格式。"><meta property="og:locale" content="zh-CN"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-07-29T00:00:00+00:00"><meta property="article:modified_time" content="2025-07-29T00:00:00+00:00"><meta property="article:tag" content="LLM"><meta property="article:tag" content="AI"><meta property="article:tag" content="JSON"><meta name=twitter:card content="summary"><meta name=twitter:title content="LLM 参数之 Response Format"><meta name=twitter:description content="深入探讨 LLM 的 Response Format 参数，如何确保生成内容符合预期格式。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://blog.cassdev.com/posts/"},{"@type":"ListItem","position":2,"name":"LLM 参数之 Response Format","item":"https://blog.cassdev.com/posts/llm-param-response-format/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"LLM 参数之 Response Format","name":"LLM 参数之 Response Format","description":"深入探讨 LLM 的 Response Format 参数，如何确保生成内容符合预期格式。","keywords":["LLM","AI","JSON"],"articleBody":"如果想让 LLM 输出 JSON 格式的内容，大家第一反应会是什么？可能大多数人和我一样，直接在提示词中写上\"请输出 JSON 格式的内容，格式为 { “key”: “value” }\"。但其实，这种方式并不是最优的。\n从之前我们也了解到了，LLM 的输出是一个概率性的文本补全器。单纯依靠提示词工程来控制 LLM 的输出格式并不可靠。用自然语言去描述一个复杂的 JSON 结构本就不易，再加上当提示词很长时，LLM 的注意力可能会分散，这些因素都容易导致它输出不符合预期的格式，甚至根本不输出 JSON。\n具体来说，这种方式可能会遇到以下三个主要问题：\n混入无关文本：模型可能在 JSON 对象前后添加对话式的\"口水话\"，如\"好的，这是您要的 JSON：…\"，这给后续的程序化解析带来了困难。\n结构性错误：生成的 JSON 可能存在语法错误，例如缺少逗号、括号不匹配或引号使用不当，导致解析失败。\n内容幻觉：模型可能\"幻觉\"出指令中未要求的字段，或遗漏必要的字段，破坏了数据模式的一致性。\n让 LLM 生成符合预期的 JSON 格式内容的最佳实践是使用 response_format 参数。这个参数允许我们直接指定输出的格式，确保 LLM 生成的内容符合预期的结构和语法。\nResponse Format 参数response_format 参数在绝大多数现代 LLM API 中都可用，允许开发者指定模型输出的格式。\nDeepSeek API Response Format\nOpenAI API Response Format\nDouBao API Response Format\n通过这个参数，我们可以明确要求 LLM 生成特定格式的内容，如 JSON 对象、纯文本或符合 JSON Schema 的数据结构。\nresponse_format 参数支持以下三个模式：\n模式 描述 text 生成纯文本内容。适用于需要自然语言回复的场景。 json_object 生成 JSON 对象。适用于需要结构化数据的场景。 json_schema 生成符合指定 JSON Schema 的 JSON 对象。适用于需要严格数据格式的场景。 使用 json_schema 模式时，需编写符合 JSON Schema 规范的模式定义，点击即刻学习 JSON Schema。\n底层原理我们都知道，Transformer 是大多数现代 LLM 的基础架构。但 response_format 参数并不作用于 Transformer 模型的内部，而是在其生成流程中加入了一个约束步骤。这个约束步骤发生在 Transformer 的 Linear 层之后、Softmax 层之前。\n下面是 Transformer 生成每个词（token）的过程：\nLinear 层 → Logits 原始分数 → Softmax 层 → 概率 -\u003e 贪婪采样 -\u003e 输出 在 Transformer 生成每个词（token）的过程中，模型首先通过 Linear 层计算出每个可能词的原始分数（Logits），然后通过 Softmax 层将这些分数转换为概率分布。接下来，模型会根据这个概率分布进行采样，选择概率最高的词作为输出。\n如果使用了 response_format 参数，模型会对 Logits 原始分数进行一轮处理，这个过程交给了 LogitsProcessor 来完成。加上 LogitsProcessor 后，Transformer 的生成流程变为：\nLinear 层 → Logits 原始分数 → Logits 处理器 -\u003e 处理后的 Logits 分数 → Softmax 层 → 概率 -\u003e 贪婪采样 -\u003e 输出 在约束生成算法的实现上，主要使用两种类型的语法：\n正则表达式（Regular Expression）：用于定义输出内容必须遵循的字符模式。通过正则表达式，我们可以精确描述诸如 JSON 对象结构等复杂格式要求。 BNF（Backus-Naur Form）：这是一种标准的上下文无关文法表示法，广泛应用于编程语言语法的精确定义，能够描述更复杂的层次化结构。 这里用大家更熟悉的正则表达式来举例说明，在应用了 response_format 参数后，LLM 输出 token 的过程如下：\nLogits 计算：Transformer 的 Linear 层为词汇表中的每个候选 token 计算出原始分数（Logits）。 约束过滤：LogitsProcessor 根据预定义的正则表达式对这些分数进行筛选，将候选 token 分为两类：符合格式要求的\"合法 token\"和不符合要求的\"非法 token\"。 分数调整：LogitsProcessor 保持合法 token 的原始分数不变，但将所有非法 token 的分数设置为负无穷（-∞）。 概率归一化：Softmax 层处理调整后的分数。由于非法 token 的分数为 -∞，经过指数函数和归一化后，它们的概率会趋近于 0。 贪婪采样：采样器根据最终的概率分布选择输出，此时只有合法 token 具有非零概率，从而确保输出符合预期格式。 下图展示了使用 Response Format 参数后，LLM 生成 token 的流程：\n笔记 更多关于 Response Format 参数的细节，可以参考 OpenAI 的官方文档 OpenAI API Structured Outputs。\n参考 Guiding LLMs The Right Way: Fast, Non-Invasive Constrained Generation\nControlling your LLM: Deep dive into Constrained Generation\nOpenAI API Response Format\n","wordCount":"238","inLanguage":"zh","datePublished":"2025-07-29T00:00:00Z","dateModified":"2025-07-29T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://blog.cassdev.com/posts/llm-param-response-format/"},"publisher":{"@type":"Organization","name":"Cassius0924 的博客","logo":{"@type":"ImageObject","url":"https://blog.cassdev.com/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://blog.cassdev.com/ accesskey=h title="Cassius0924 的博客 (Alt + H)">Cassius0924 的博客</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://blog.cassdev.com/en/ title=English aria-label=English>English</a></li></ul></div></div><ul id=menu><li><a href=https://blog.cassdev.com/ title="Cassius0924 的博客"><span>首页</span></a></li><li><a href=https://blog.cassdev.com/search/ title="搜索 (Alt + /)" accesskey=/><span>搜索</span></a></li><li><a href=https://blog.cassdev.com/mind-space/ title=思维空间><span>思维空间</span></a></li><li><a href=https://blog.cassdev.com/tags/ title=标签><span>标签</span></a></li><li><a href=https://blog.cassdev.com/archives/ title=归档><span>归档</span></a></li><li><a href=https://blog.cassdev.com/friends/ title=友链><span>友链</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://blog.cassdev.com/>主页</a>&nbsp;»&nbsp;<a href=https://blog.cassdev.com/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">LLM 参数之 Response Format</h1><div class=post-description>深入探讨 LLM 的 Response Format 参数，如何确保生成内容符合预期格式。</div><div class=post-meta><span title='2025-07-29 00:00:00 +0000 UTC'>2025年07月29日</span>&nbsp;·&nbsp;2 分钟</div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>目录</span></summary><div class=inner><ul><li><a href=#response-format-%e5%8f%82%e6%95%b0 aria-label="Response Format 参数">Response Format 参数</a></li><li><a href=#%e5%ba%95%e5%b1%82%e5%8e%9f%e7%90%86 aria-label=底层原理>底层原理</a></li><li><a href=#%e5%8f%82%e8%80%83 aria-label=参考>参考</a></li></ul></div></details></div><div class=post-content><p>如果想让 LLM 输出 JSON 格式的内容，大家第一反应会是什么？可能大多数人和我一样，直接在提示词中写上"请输出 JSON 格式的内容，格式为 { &ldquo;key&rdquo;: &ldquo;value&rdquo; }"。但其实，这种方式并不是最优的。</p><p>从之前我们也了解到了，LLM 的输出是一个概率性的文本补全器。单纯依靠提示词工程来控制 LLM 的输出格式并不可靠。用自然语言去描述一个复杂的 JSON 结构本就不易，再加上当提示词很长时，LLM 的注意力可能会分散，这些因素都容易导致它输出不符合预期的格式，甚至根本不输出 JSON。</p><p>具体来说，这种方式可能会遇到以下三个主要问题：</p><ol><li><p><strong>混入无关文本</strong>：模型可能在 JSON 对象前后添加对话式的"口水话"，如"好的，这是您要的 JSON：&mldr;"，这给后续的程序化解析带来了困难。</p></li><li><p><strong>结构性错误</strong>：生成的 JSON 可能存在语法错误，例如缺少逗号、括号不匹配或引号使用不当，导致解析失败。</p></li><li><p><strong>内容幻觉</strong>：模型可能"幻觉"出指令中未要求的字段，或遗漏必要的字段，破坏了数据模式的一致性。</p></li></ol><p>让 LLM 生成符合预期的 JSON 格式内容的最佳实践是使用 <code>response_format</code> 参数。这个参数允许我们直接指定输出的格式，确保 LLM 生成的内容符合预期的结构和语法。</p><h2 id=response-format-参数>Response Format 参数<a hidden class=anchor aria-hidden=true href=#response-format-参数>#</a></h2><p><code>response_format</code> 参数在绝大多数现代 LLM API 中都可用，允许开发者指定模型输出的格式。</p><ul><li><p><a href=https://api-docs.deepseek.com/zh-cn/api/create-chat-completion#request>DeepSeek API Response Format</a></p></li><li><p><a href=https://platform.openai.com/docs/api-reference/chat/create#chat-create-response_format>OpenAI API Response Format</a></p></li><li><p><a href=https://www.volcengine.com/docs/82379/1494384>DouBao API Response Format</a></p></li></ul><p>通过这个参数，我们可以明确要求 LLM 生成特定格式的内容，如 JSON 对象、纯文本或符合 JSON Schema 的数据结构。</p><p><code>response_format</code> 参数支持以下三个模式：</p><table><thead><tr><th style=text-align:left>模式</th><th style=text-align:left>描述</th></tr></thead><tbody><tr><td style=text-align:left><code>text</code></td><td style=text-align:left>生成纯文本内容。适用于需要自然语言回复的场景。</td></tr><tr><td style=text-align:left><code>json_object</code></td><td style=text-align:left>生成 JSON 对象。适用于需要结构化数据的场景。</td></tr><tr><td style=text-align:left><code>json_schema</code></td><td style=text-align:left>生成符合指定 JSON Schema 的 JSON 对象。适用于需要严格数据格式的场景。</td></tr></tbody></table><p>使用 <code>json_schema</code> 模式时，需编写符合 JSON Schema 规范的模式定义，点击即刻学习 <a href=https://json-schema.apifox.cn/>JSON Schema</a>。</p><h2 id=底层原理>底层原理<a hidden class=anchor aria-hidden=true href=#底层原理>#</a></h2><p>我们都知道，Transformer 是大多数现代 LLM 的基础架构。但 <code>response_format</code> 参数并不作用于 Transformer 模型的内部，而是在其生成流程中加入了一个约束步骤。这个约束步骤发生在 Transformer 的 Linear 层之后、Softmax 层之前。</p><p><img alt="The Transformer With Logits Processor" loading=lazy src=https://s2.loli.net/2025/07/29/7vDU5qsMrR3aFbY.png></p><p>下面是 Transformer 生成每个词（token）的过程：</p><pre tabindex=0><code>Linear 层 → Logits 原始分数 → Softmax 层 → 概率 -&gt; 贪婪采样 -&gt; 输出
</code></pre><p>在 Transformer 生成每个词（token）的过程中，模型首先通过 Linear 层计算出每个可能词的原始分数（Logits），然后通过 Softmax 层将这些分数转换为概率分布。接下来，模型会根据这个概率分布进行采样，选择概率最高的词作为输出。</p><p>如果使用了 <code>response_format</code> 参数，模型会对 Logits 原始分数进行一轮处理，这个过程交给了 LogitsProcessor 来完成。加上 LogitsProcessor 后，Transformer 的生成流程变为：</p><pre tabindex=0><code>Linear 层 → Logits 原始分数 → Logits 处理器 -&gt; 处理后的 Logits 分数 → Softmax 层 → 概率 -&gt; 贪婪采样 -&gt; 输出
</code></pre><p>在约束生成算法的实现上，主要使用两种类型的语法：</p><ul><li><strong>正则表达式（Regular Expression）</strong>：用于定义输出内容必须遵循的字符模式。通过正则表达式，我们可以精确描述诸如 JSON 对象结构等复杂格式要求。</li><li><strong>BNF（Backus-Naur Form）</strong>：这是一种标准的上下文无关文法表示法，广泛应用于编程语言语法的精确定义，能够描述更复杂的层次化结构。</li></ul><p>这里用大家更熟悉的正则表达式来举例说明，在应用了 <code>response_format</code> 参数后，LLM 输出 token 的过程如下：</p><ol><li><strong>Logits 计算</strong>：Transformer 的 Linear 层为词汇表中的每个候选 token 计算出原始分数（Logits）。</li><li><strong>约束过滤</strong>：LogitsProcessor 根据预定义的正则表达式对这些分数进行筛选，将候选 token 分为两类：符合格式要求的"合法 token"和不符合要求的"非法 token"。</li><li><strong>分数调整</strong>：LogitsProcessor 保持合法 token 的原始分数不变，但将所有非法 token 的分数设置为负无穷（-∞）。</li><li><strong>概率归一化</strong>：Softmax 层处理调整后的分数。由于非法 token 的分数为 -∞，经过指数函数和归一化后，它们的概率会趋近于 0。</li><li><strong>贪婪采样</strong>：采样器根据最终的概率分布选择输出，此时只有合法 token 具有非零概率，从而确保输出符合预期格式。</li></ol><p>下图展示了使用 Response Format 参数后，LLM 生成 token 的流程：</p><p><img alt="Response Format Generation Example" loading=lazy src=https://s2.loli.net/2025/07/29/tOJF1ve5ZR6Td8l.png></p><link rel=stylesheet href=/css/vendors/admonitions.4fd9a0b8ec8899f2ca952048d255a569f433f77dfb3f52f5bc87e7d65cdce449.css integrity="sha256-T9mguOyImfLKlSBI0lWlafQz9337P1L1vIfn1lzc5Ek=" crossorigin=anonymous><div class="admonition note"><div class=admonition-header><svg viewBox="0 0 576 512"><path d="M0 64C0 28.7 28.7.0 64 0H224v128c0 17.7 14.3 32 32 32h128v125.7l-86.8 86.8c-10.3 10.3-17.5 23.1-21 37.2l-18.7 74.9c-2.3 9.2-1.8 18.8 1.3 27.5L64 512c-35.3.0-64-28.7-64-64V64zm384 64H256V0L384 128zM549.8 235.7l14.4 14.4c15.6 15.6 15.6 40.9.0 56.6l-29.4 29.4-71-71 29.4-29.4c15.6-15.6 40.9-15.6 56.6.0zM311.9 417 441.1 287.8l71 71L382.9 487.9c-4.1 4.1-9.2 7-14.9 8.4l-60.1 15c-5.5 1.4-11.2-.2-15.2-4.2s-5.6-9.7-4.2-15.2l15-60.1c1.4-5.6 4.3-10.8 8.4-14.9z"/></svg>
<span>笔记</span></div><div class=admonition-content><p>更多关于 Response Format 参数的细节，可以参考 OpenAI 的官方文档 <a href="https://platform.openai.com/docs/guides/structured-outputs?api-mode=responses">OpenAI API Structured Outputs</a>。</p></div></div><h2 id=参考>参考<a hidden class=anchor aria-hidden=true href=#参考>#</a></h2><ul><li><p><a href=https://medium.com/@docherty/controlling-your-llm-deep-dive-into-constrained-generation-1e561c736a20>Guiding LLMs The Right Way: Fast, Non-Invasive Constrained Generation</a></p></li><li><p><a href=https://medium.com/@docherty/controlling-your-llm-deep-dive-into-constrained-generation-1e561c736a20>Controlling your LLM: Deep dive into Constrained Generation</a></p></li><li><p><a href=https://platform.openai.com/docs/api-reference/chat/create#chat-create-response_format>OpenAI API Response Format</a></p></li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://blog.cassdev.com/tags/llm/>LLM</a></li><li><a href=https://blog.cassdev.com/tags/ai/>AI</a></li><li><a href=https://blog.cassdev.com/tags/json/>JSON</a></li></ul></footer></article></main><footer class=footer><span>Copyright © 2025-2025 Cassius0924. All rights reserved.</span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="复制";function s(){t.innerHTML="已复制！",setTimeout(()=>{t.innerHTML="复制"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>