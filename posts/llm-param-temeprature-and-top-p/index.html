<!doctype html><html lang=zh dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>LLM 参数之 Temperature 和 Top-p | Cassius0924 的博客</title><meta name=keywords content="LLM,AI"><meta name=description content="深入探讨AI生成内容中的Temperature和Top-p参数的作用与实现机制。"><meta name=author content="Cassius0924"><link rel=canonical href=https://blog.cassdev.com/posts/llm-param-temeprature-and-top-p/><link crossorigin=anonymous href=/assets/css/stylesheet.4f5249ac184c033613d94c5267209e90161d3a93365c2feb0ad3960a4bbb6393.css integrity="sha256-T1JJrBhMAzYT2UxSZyCekBYdOpM2XC/rCtOWCku7Y5M=" rel="preload stylesheet" as=style><link rel=icon href=https://blog.cassdev.com/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://blog.cassdev.com/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://blog.cassdev.com/favicon-32x32.png><link rel=apple-touch-icon href=https://blog.cassdev.com/apple-touch-icon.png><link rel=mask-icon href=https://blog.cassdev.com/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=zh href=https://blog.cassdev.com/posts/llm-param-temeprature-and-top-p/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://blog.cassdev.com/posts/llm-param-temeprature-and-top-p/"><meta property="og:site_name" content="Cassius0924 的博客"><meta property="og:title" content="LLM 参数之 Temperature 和 Top-p"><meta property="og:description" content="深入探讨AI生成内容中的Temperature和Top-p参数的作用与实现机制。"><meta property="og:locale" content="zh-CN"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-07-31T00:00:00+00:00"><meta property="article:modified_time" content="2025-07-31T00:00:00+00:00"><meta property="article:tag" content="LLM"><meta property="article:tag" content="AI"><meta name=twitter:card content="summary"><meta name=twitter:title content="LLM 参数之 Temperature 和 Top-p"><meta name=twitter:description content="深入探讨AI生成内容中的Temperature和Top-p参数的作用与实现机制。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://blog.cassdev.com/posts/"},{"@type":"ListItem","position":2,"name":"LLM 参数之 Temperature 和 Top-p","item":"https://blog.cassdev.com/posts/llm-param-temeprature-and-top-p/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"LLM 参数之 Temperature 和 Top-p","name":"LLM 参数之 Temperature 和 Top-p","description":"深入探讨AI生成内容中的Temperature和Top-p参数的作用与实现机制。","keywords":["LLM","AI"],"articleBody":"大家在使用 LLM 生成内容时，不知道有没有注意到 LLM 的一些可配置参数，比如 Temperature 和 Top-p，是否关注过这些参数的作用？\n无论是在 OpenAI 的 API 文档、Google 的 AI Studio、以及各种的 AI 平台，你都能看到它的身影。\n什么是 Temperature 和 Top-p？在与 LLM 聊天时，大家可能已经注意到，有的 Agent 十分有创造力，有的 Agent 又十分严谨。这其中除了 Prompt 的影响外，还有一个重要的因素就是 LLM 的采样参数，包括 Temperature 和 Top-p。\n提示 TL;DR\n二者都是用于控制 LLM 生成内容随机性的参数。它们就像旋钮，通过调节它们来决定 LLM 输出的“保守”程度或“冒险”程度。\ntemperature 通过调整 token 生成的概率分布来控制输出的随机性 top_p 通过限制 LLM 考虑的 token 范围来控制输出的随机性 一个简单的例子为了更好地理解 temperature 和 top_p 如何影响概率，让我们想象一个场景：\n假设 LLM 正在生成一句话：“我想吃___。”，下面是 LLM 待选的 token 列表：\n水果 (60%) 零食 (20%) 饺子 (10%) … 大蒜 (0.01%) 螺丝 (0.001%) 汽油 (0.0001%) Temperature: 在默认情况下（temperature=1），LLM 很可能会选择“水果”或“零食”。但如果我们提高 temperature -\u003e 1.7，那些低概率的选项，如“大蒜”或“螺丝”，被选中的可能性就会增加，从而让句子变得出人意料。反之，如果我们降低 temperature -\u003e 0.2，LLM 几乎只会选择“水果”，输出会更为确定。\nTop-p: 默认 top_p=1 时，LLM 会考虑所有 token 的概率分布，也就是说 LLM 存在选择“汽油”或“螺丝”的可能性。但如果我们设置 top_p=0.85，LLM 只会考虑累计概率达到 85% 的最小 token 集合，也就是“水果”+“零食”+“饺子”=90% \u003e= 85%，这样 LLM 就只会在这三个选项中进行选择，输出更加符合常理。\nTransformer 的 Linear 层和 Softmax 层要理解 temperature 和 top_p 的底层原理，我们需要简单了解一下 Transformer 模型（大多数现代 LLM 的基础架构）的内部机制。\n想必大家都见过这张 Transformer 的架构图，它自 LLM 的开山鼻祖论文 Attention Is All You Need。可以看到在架构图的输出端最后接了一个 Linear 层和 Softmax 层，这就是模型生成每个词的概率分布的地方。\nLinear 层解码器的输出会经过 Linear 层，由 Linear 层将 LLM 的内部表示翻译每个 token 的原始分数，称为 Logits。每个 logit 值代表了 LLM 对于相应 token 是句子中下一个正确 token 的置信度（该词作为下一个词的“合理性”）。该层有效地将模型内部的、抽象的语义表征“翻译”为在整个 token 词汇表上的具体预测。\nSoftmax 层Softmax 主要是将 Linear 层输出的 logits 转换为一个概率分布。这一转换过程分为两个步骤：\n指数化：对每个 logit 应用指数函数，得到一个非负值。 归一化：将所有指数化后的值进行归一化处理，使得它们的总和为 1，从而形成一个概率分布。 Softmax 的具体数学模型如下：\n$$ P(token_i) = softmax(logit_i) = \\frac{e^{logit_i}}{\\sum_{j} e^{logit_j}} $$\n其中：\n$P(token_i)$ 是第 $i$ 个词的最终概率。 $logit_i$ 是第 $i$ 个词的原始分数。 笔记 插曲：Softmax 为什么叫 Softmax？\n除了 Softmax 函数，还有一种叫 Hardmax 的函数。\nHardmax（严格 argmax）：会将最大元素对应位置输出 1，其它位置输出 0； Softmax（平滑 argmax）：最大元素会获得更高但不绝对的概率，其它元素也会有一定概率。 所以 Softmax 的意思是“平滑的最大化参数”，它允许模型在生成时考虑多个可能的选项，而不是仅仅选择一个最可能的选项。\n只有使用 Softmax 函数，才能将原始分数转换为概率分布。而 Hardmax 函数则会将最大分数对应的词的概率设为 1，其它词的概率设为 0，这样就没有随机性了。\nTemperature 和 Top-p 的底层实现Temperature 的底层实现现在我们了解了 Logits 是什么，以及 Softmax 函数的作用。那么就可以来看看 temperature 是起作用的了。\n从上面的例子中了解到 temperature 参数影响着概率分布的平滑程度。它的实现方式就是对 logits 进行缩放。引入 temperature 参数后，Softmax 函数的公式变为：\n$$ P(token_i) = softmax(\\frac{logit_i}{T}) $$\n其中 $T$ 是 temperature 参数。\nSoftmax 函数能够反映出 logits 的差异程度。而 logits 除以 temperature 后，由于除法的特性，若 $T \u003c 1$，则会放大 logits 的差异，使得高分词的概率更高，低分词的概率更低；若 $T \u003e 1$，则会缩小 logits 的差异，使得低分词的概率被提升。\n笔记 Temperature 的范围是 (0, 2]\n所以这就能看出为什么 temperature 值大小范围是在 (0, 2] 之间，因为使用的是除法，只有当 T 小于 1 时，才会使得高分词的概率更高，而当 T 大于 1 时，低分词的概率才会被提升。但又不能让 T 无限制增大，否则就会导致输出完全随机，LLM 也就失去了意义。\nTop-p 的底层实现top_p 的实现方式显而易见了，分为以下几个步骤：\n排序：将所有 token 按照概率从高到低排序。 累加：从概率最高的 token 开始，逐个累加它们的概率，直到累计概率大于等于 top_p 的值。 截断：只保留那些累计概率在 top_p 范围内的 token（包括最后一个超过 top_p 的 token）。 重新归一化：对保留的 token 的概率进行归一化处理，使它们的总和为 1。 Temperature 和 Top-p 设置参考不同的任务需要不同的创造力 or 保守程度。以下是一些常见的 temperature 和 top_p 设置建议：\n用例 Temperature Top-p 描述 代码生成 0.2 0.1 生成符合既定模式和约定的代码。输出更具确定性和针对性。有助于生成语法正确的代码。 创意写作 0.7 0.8 生成富有创意且多样化的叙事文本。输出更具探索性，且不受模式限制。 聊天机器人回复 0.5 0.5 生成兼顾连贯性和多样性的对话回复。输出结果更加自然、引人入胜。 代码注释生成 0.3 0.2 生成更简洁、更相关的代码注释。输出更具确定性，并符合规范。 数据分析脚本 0.2 0.1 生成更准确、更高效的数据分析脚本。输出结果更具确定性和针对性。 表格参考自 Cheat Sheet: Mastering Temperature and Top-p in ChatGPT API\n二者的执行顺序网络上很少关于 temperature 和 top_p 的执行顺序的资料。所以直接找到 Transformer 源码老家，看看具体实现。\n定位到 _get_logits_warper 函数，这个函数负责处理 logits 的变换，这里按顺序往一个 warper（TFLogitsProcessorList 的实例）中插入了 temperature、top_k 和 top_p 的处理器（TF***LogitsWarper）。\n接着看一下 TFLogitsProcessorList 这个类的方法，发现只有 __call__ 方法，也就是可以将该 warper 进行调用，具体逻辑是使用 for 循环按序调用每个处理器。\n所以，temperature 和 top_p 两个参数的应用顺序为：先应用 temperature，再应用 top_p。\n总结temperature 是一个强大而直观的工具，它让我们能够精细地控制 AI 的创造力。通过理解其背后的原理并学会在合适的场景下应用它，我们不仅能让 AI 的输出更符合我们的预期，还能解锁其在创意和探索方面的巨大潜力。下次当你与 AI 互动时，不妨尝试调整一下这个“温度”旋钮，看看会发生什么奇妙的变化。\n对业务的思考在实际业务中，作为 LLM 应用层开发者的我们不只有 All in Prompt Engineering 这条路来提升模型表现，也可以利用 temperature 这类采样参数进行精细化调控。像 LangChain、Eino 等框架，都提供了调整 temperature、top_p 等参数的能力（详见文档）。 在构建多 Agent 系统时，我们要对每个 Agent 的职责进行明确划分，并对其采样参数进行量身定制，做到控制 Agent 的输出风格和内容质量。\n参考 https://www.mdrk.io/temperature-samplig-in-ai/ https://community.openai.com/t/cheat-sheet-mastering-temperature-and-top-p-in-chatgpt-api/172683 https://www.bilibili.com/video/BV1xmA2eMEFF/?spm_id_from=333.337.search-card.all.click\u0026vd_source=77e39221f40d20651d3556c53a9c7622 ","wordCount":"412","inLanguage":"zh","datePublished":"2025-07-31T00:00:00Z","dateModified":"2025-07-31T00:00:00Z","author":{"@type":"Person","name":"Cassius0924"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://blog.cassdev.com/posts/llm-param-temeprature-and-top-p/"},"publisher":{"@type":"Organization","name":"Cassius0924 的博客","logo":{"@type":"ImageObject","url":"https://blog.cassdev.com/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://blog.cassdev.com/ accesskey=h title="Cassius0924 的博客 (Alt + H)">Cassius0924 的博客</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://blog.cassdev.com/en/ title=English aria-label=English>English</a></li></ul></div></div><ul id=menu><li><a href=https://blog.cassdev.com/ title="Cassius0924 的博客"><span>首页</span></a></li><li><a href=https://blog.cassdev.com/search/ title="搜索 (Alt + /)" accesskey=/><span>搜索</span></a></li><li><a href=https://blog.cassdev.com/mind-space/ title=思维空间><span>思维空间</span></a></li><li><a href=https://blog.cassdev.com/tags/ title=标签><span>标签</span></a></li><li><a href=https://blog.cassdev.com/archives/ title=归档><span>归档</span></a></li><li><a href=https://blog.cassdev.com/friends/ title=友链><span>友链</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://blog.cassdev.com/>主页</a>&nbsp;»&nbsp;<a href=https://blog.cassdev.com/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">LLM 参数之 Temperature 和 Top-p</h1><div class=post-description>深入探讨AI生成内容中的Temperature和Top-p参数的作用与实现机制。</div><div class=post-meta><span title='2025-07-31 00:00:00 +0000 UTC'>2025年07月31日</span>&nbsp;·&nbsp;2 分钟&nbsp;·&nbsp;Cassius0924</div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>目录</span></summary><div class=inner><ul><li><a href=#%e4%bb%80%e4%b9%88%e6%98%af-temperature-%e5%92%8c-top-p aria-label="什么是 Temperature 和 Top-p？">什么是 Temperature 和 Top-p？</a></li><li><a href=#%e4%b8%80%e4%b8%aa%e7%ae%80%e5%8d%95%e7%9a%84%e4%be%8b%e5%ad%90 aria-label=一个简单的例子>一个简单的例子</a></li><li><a href=#transformer-%e7%9a%84-linear-%e5%b1%82%e5%92%8c-softmax-%e5%b1%82 aria-label="Transformer 的 Linear 层和 Softmax 层">Transformer 的 Linear 层和 Softmax 层</a><ul><li><a href=#linear-%e5%b1%82 aria-label="Linear 层">Linear 层</a></li><li><a href=#softmax-%e5%b1%82 aria-label="Softmax 层">Softmax 层</a></li></ul></li><li><a href=#temperature-%e5%92%8c-top-p-%e7%9a%84%e5%ba%95%e5%b1%82%e5%ae%9e%e7%8e%b0 aria-label="Temperature 和 Top-p 的底层实现">Temperature 和 Top-p 的底层实现</a><ul><li><a href=#temperature-%e7%9a%84%e5%ba%95%e5%b1%82%e5%ae%9e%e7%8e%b0 aria-label="Temperature 的底层实现">Temperature 的底层实现</a></li><li><a href=#top-p-%e7%9a%84%e5%ba%95%e5%b1%82%e5%ae%9e%e7%8e%b0 aria-label="Top-p 的底层实现">Top-p 的底层实现</a></li><li><a href=#temperature-%e5%92%8c-top-p-%e8%ae%be%e7%bd%ae%e5%8f%82%e8%80%83 aria-label="Temperature 和 Top-p 设置参考">Temperature 和 Top-p 设置参考</a></li><li><a href=#%e4%ba%8c%e8%80%85%e7%9a%84%e6%89%a7%e8%a1%8c%e9%a1%ba%e5%ba%8f aria-label=二者的执行顺序>二者的执行顺序</a></li><li><a href=#%e6%80%bb%e7%bb%93 aria-label=总结>总结</a></li><li><a href=#%e5%af%b9%e4%b8%9a%e5%8a%a1%e7%9a%84%e6%80%9d%e8%80%83 aria-label=对业务的思考>对业务的思考</a></li></ul></li><li><a href=#%e5%8f%82%e8%80%83 aria-label=参考>参考</a></li></ul></div></details></div><div class=post-content><p>大家在使用 LLM 生成内容时，不知道有没有注意到 LLM 的一些可配置参数，比如 Temperature 和 Top-p，是否关注过这些参数的作用？</p><p>无论是在 OpenAI 的 API 文档、Google 的 AI Studio、以及各种的 AI 平台，你都能看到它的身影。</p><p><img alt="OpenAI Doc" loading=lazy src=https://s2.loli.net/2025/07/22/ofLOVJnXuImeW3Z.png>
<img alt="Google AI Studio" loading=lazy src=https://s2.loli.net/2025/07/22/1jlwDWpBJ6kxefu.png></p><h2 id=什么是-temperature-和-top-p>什么是 Temperature 和 Top-p？<a hidden class=anchor aria-hidden=true href=#什么是-temperature-和-top-p>#</a></h2><p>在与 LLM 聊天时，大家可能已经注意到，有的 Agent 十分有创造力，有的 Agent 又十分严谨。这其中除了 Prompt 的影响外，还有一个重要的因素就是 LLM 的<strong>采样参数</strong>，包括 Temperature 和 Top-p。</p><link rel=stylesheet href=/css/vendors/admonitions.4fd9a0b8ec8899f2ca952048d255a569f433f77dfb3f52f5bc87e7d65cdce449.css integrity="sha256-T9mguOyImfLKlSBI0lWlafQz9337P1L1vIfn1lzc5Ek=" crossorigin=anonymous><div class="admonition tip"><div class=admonition-header><svg viewBox="0 0 384 512"><path d="M272 384c9.6-31.9 29.5-59.1 49.2-86.2 5.2-7.1 10.4-14.2 15.4-21.4 19.8-28.5 31.4-63 31.4-100.3C368 78.8 289.2.0 192 0S16 78.8 16 176c0 37.3 11.6 71.9 31.4 100.3 5 7.2 10.2 14.3 15.4 21.4 19.8 27.1 39.7 54.4 49.2 86.2h160zM192 512c44.2.0 80-35.8 80-80v-16H112v16c0 44.2 35.8 80 80 80zM112 176c0 8.8-7.2 16-16 16s-16-7.2-16-16c0-61.9 50.1-112 112-112 8.8.0 16 7.2 16 16s-7.2 16-16 16c-44.2.0-80 35.8-80 80z"/></svg>
<span>提示</span></div><div class=admonition-content><p><strong>TL;DR</strong></p><p>二者都是用于控制 LLM 生成内容<strong>随机性</strong>的参数。它们就像旋钮，通过调节它们来决定 LLM 输出的“保守”程度或“冒险”程度。</p><ul><li><code>temperature</code> 通过调整 token 生成的概率分布来控制输出的随机性</li><li><code>top_p</code> 通过限制 LLM 考虑的 token 范围来控制输出的随机性</li></ul></div></div><h2 id=一个简单的例子>一个简单的例子<a hidden class=anchor aria-hidden=true href=#一个简单的例子>#</a></h2><p>为了更好地理解 <code>temperature</code> 和 <code>top_p</code> 如何影响概率，让我们想象一个场景：</p><p>假设 LLM 正在生成一句话：<em>“我想吃___。”</em>，下面是 LLM 待选的 token 列表：</p><ul><li>水果 (60%)</li><li>零食 (20%)</li><li>饺子 (10%)</li><li>&mldr;</li><li>大蒜 (0.01%)</li><li>螺丝 (0.001%)</li><li>汽油 (0.0001%)</li></ul><p><img alt="LLM Token Sampling Example" loading=lazy src=https://s2.loli.net/2025/08/01/5Lcnt2b9NJU6r1x.png></p><ul><li><p><strong>Temperature</strong>: 在默认情况下（<code>temperature=1</code>），LLM 很可能会选择“水果”或“零食”。但如果我们<strong>提高</strong> <code>temperature -> 1.7</code>，那些低概率的选项，如“大蒜”或“螺丝”，被选中的可能性就会增加，从而让句子变得出人意料。反之，如果我们<strong>降低</strong> <code>temperature -> 0.2</code>，LLM 几乎只会选择“水果”，输出会更为确定。</p></li><li><p><strong>Top-p</strong>: 默认 <code>top_p=1</code> 时，LLM 会考虑所有 token 的概率分布，也就是说 LLM 存在选择“汽油”或“螺丝”的可能性。但如果我们设置 <code>top_p=0.85</code>，LLM 只会考虑累计概率达到 85% 的最小 token 集合，也就是“水果”+“零食”+“饺子”=90% >= 85%，这样 LLM 就只会在这三个选项中进行选择，输出更加符合常理。</p></li></ul><h2 id=transformer-的-linear-层和-softmax-层>Transformer 的 Linear 层和 Softmax 层<a hidden class=anchor aria-hidden=true href=#transformer-的-linear-层和-softmax-层>#</a></h2><p>要理解 <code>temperature</code> 和 <code>top_p</code> 的底层原理，我们需要简单了解一下 Transformer 模型（大多数现代 LLM 的基础架构）的内部机制。</p><p>想必大家都见过这张 Transformer 的架构图，它自 LLM 的开山鼻祖论文 <a href=https://arxiv.org/abs/1706.03762>Attention Is All You Need</a>。可以看到在架构图的输出端最后接了一个 Linear 层和 Softmax 层，这就是模型生成每个词的概率分布的地方。</p><p><img alt="The Transformer" loading=lazy src=https://s2.loli.net/2025/07/22/pWDzorT6wvlJCSe.png></p><h3 id=linear-层>Linear 层<a hidden class=anchor aria-hidden=true href=#linear-层>#</a></h3><p>解码器的输出会经过 Linear 层，由 Linear 层将 LLM 的内部表示翻译每个 token 的<strong>原始分数</strong>，称为 <strong>Logits</strong>。每个 logit 值代表了 LLM 对于相应 token 是句子中下一个正确 token 的<strong>置信度</strong>（该词作为下一个词的“合理性”）。该层有效地将模型内部的、抽象的语义表征“翻译”为在整个 token 词汇表上的具体预测。</p><p><img alt="Linear Layer" loading=lazy src=https://s2.loli.net/2025/07/30/6gcxYj3frhZJDtC.png></p><h3 id=softmax-层>Softmax 层<a hidden class=anchor aria-hidden=true href=#softmax-层>#</a></h3><p>Softmax 主要是将 Linear 层输出的 logits 转换为一个<strong>概率分布</strong>。这一转换过程分为两个步骤：</p><ol><li><strong>指数化</strong>：对每个 logit 应用指数函数，得到一个非负值。</li><li><strong>归一化</strong>：将所有指数化后的值进行归一化处理，使得它们的总和为 1，从而形成一个概率分布。</li></ol><p>Softmax 的具体数学模型如下：</p><p>$$
P(token_i) = softmax(logit_i) = \frac{e^{logit_i}}{\sum_{j} e^{logit_j}}
$$</p><p>其中：</p><ul><li>$P(token_i)$ 是第 $i$ 个词的最终概率。</li><li>$logit_i$ 是第 $i$ 个词的原始分数。</li></ul><p><img alt="Softmax Layer" loading=lazy src=https://s2.loli.net/2025/07/30/Kduwar1DE4kBzYn.png></p><div class="admonition note"><div class=admonition-header><svg viewBox="0 0 576 512"><path d="M0 64C0 28.7 28.7.0 64 0H224v128c0 17.7 14.3 32 32 32h128v125.7l-86.8 86.8c-10.3 10.3-17.5 23.1-21 37.2l-18.7 74.9c-2.3 9.2-1.8 18.8 1.3 27.5L64 512c-35.3.0-64-28.7-64-64V64zm384 64H256V0L384 128zM549.8 235.7l14.4 14.4c15.6 15.6 15.6 40.9.0 56.6l-29.4 29.4-71-71 29.4-29.4c15.6-15.6 40.9-15.6 56.6.0zM311.9 417 441.1 287.8l71 71L382.9 487.9c-4.1 4.1-9.2 7-14.9 8.4l-60.1 15c-5.5 1.4-11.2-.2-15.2-4.2s-5.6-9.7-4.2-15.2l15-60.1c1.4-5.6 4.3-10.8 8.4-14.9z"/></svg>
<span>笔记</span></div><div class=admonition-content><p><strong>插曲：Softmax 为什么叫 Softmax？</strong></p><p>除了 Softmax 函数，还有一种叫 Hardmax 的函数。</p><ul><li>Hardmax（严格 argmax）：会将最大元素对应位置输出 1，其它位置输出 0；</li><li>Softmax（平滑 argmax）：最大元素会获得更高但不绝对的概率，其它元素也会有一定概率。</li></ul><p>所以 Softmax 的意思是“平滑的最大化参数”，它允许模型在生成时考虑多个可能的选项，而不是仅仅选择一个最可能的选项。</p><p>只有使用 Softmax 函数，才能将原始分数转换为概率分布。而 Hardmax 函数则会将最大分数对应的词的概率设为 1，其它词的概率设为 0，这样就没有随机性了。</p></div></div><h2 id=temperature-和-top-p-的底层实现>Temperature 和 Top-p 的底层实现</h2><h3 id=temperature-的底层实现>Temperature 的底层实现<a hidden class=anchor aria-hidden=true href=#temperature-和-top-p-的底层实现>#</a></h3><p>现在我们了解了 Logits 是什么，以及 Softmax 函数的作用。那么就可以来看看 <code>temperature</code> 是起作用的了。</p><p>从上面的例子中了解到 <code>temperature</code> 参数影响着概率分布的平滑程度。它的实现方式就是对 logits 进行缩放。引入 <code>temperature</code> 参数后，Softmax 函数的公式变为：</p><p>$$
P(token_i) = softmax(\frac{logit_i}{T})
$$</p><p>其中 $T$ 是 <code>temperature</code> 参数。</p><p>Softmax 函数能够反映出 logits 的差异程度。而 logits 除以 temperature 后，由于除法的特性，若 $T &lt; 1$，则会放大 logits 的差异，使得高分词的概率更高，低分词的概率更低；若 $T > 1$，则会缩小 logits 的差异，使得低分词的概率被提升。</p><div class="admonition note"><div class=admonition-header><svg viewBox="0 0 576 512"><path d="M0 64C0 28.7 28.7.0 64 0H224v128c0 17.7 14.3 32 32 32h128v125.7l-86.8 86.8c-10.3 10.3-17.5 23.1-21 37.2l-18.7 74.9c-2.3 9.2-1.8 18.8 1.3 27.5L64 512c-35.3.0-64-28.7-64-64V64zm384 64H256V0L384 128zM549.8 235.7l14.4 14.4c15.6 15.6 15.6 40.9.0 56.6l-29.4 29.4-71-71 29.4-29.4c15.6-15.6 40.9-15.6 56.6.0zM311.9 417 441.1 287.8l71 71L382.9 487.9c-4.1 4.1-9.2 7-14.9 8.4l-60.1 15c-5.5 1.4-11.2-.2-15.2-4.2s-5.6-9.7-4.2-15.2l15-60.1c1.4-5.6 4.3-10.8 8.4-14.9z"/></svg>
<span>笔记</span></div><div class=admonition-content><p><strong>Temperature 的范围是 (0, 2]</strong></p><p>所以这就能看出为什么 <code>temperature</code> 值大小范围是在 (0, 2] 之间，因为使用的是除法，只有当 <code>T</code> 小于 1 时，才会使得高分词的概率更高，而当 <code>T</code> 大于 1 时，低分词的概率才会被提升。但又不能让 <code>T</code> 无限制增大，否则就会导致输出完全随机，LLM 也就失去了意义。</p></div></div><h3 id=top-p-的底层实现>Top-p 的底层实现<a hidden class=anchor aria-hidden=true href=#top-p-的底层实现>#</a></h3><p><code>top_p</code> 的实现方式显而易见了，分为以下几个步骤：</p><ol><li><strong>排序</strong>：将所有 token 按照概率从高到低排序。</li><li><strong>累加</strong>：从概率最高的 token 开始，逐个累加它们的概率，直到累计概率大于等于 <code>top_p</code> 的值。</li><li><strong>截断</strong>：只保留那些累计概率在 <code>top_p</code> 范围内的 token（包括最后一个超过 <code>top_p</code> 的 token）。</li><li><strong>重新归一化</strong>：对保留的 token 的概率进行归一化处理，使它们的总和为 1。</li></ol><p><img alt="Top-p Principles" loading=lazy src=https://s2.loli.net/2025/08/01/opyeujkR61W8SbL.png></p><h3 id=temperature-和-top-p-设置参考>Temperature 和 Top-p 设置参考<a hidden class=anchor aria-hidden=true href=#temperature-和-top-p-设置参考>#</a></h3><p>不同的任务需要不同的<strong>创造力</strong> or <strong>保守程度</strong>。以下是一些常见的 <code>temperature</code> 和 <code>top_p</code> 设置建议：</p><table><thead><tr><th style=text-align:left>用例</th><th style=text-align:left>Temperature</th><th style=text-align:left>Top-p</th><th>描述</th></tr></thead><tbody><tr><td style=text-align:left>代码生成</td><td style=text-align:left>0.2</td><td style=text-align:left>0.1</td><td>生成符合既定模式和约定的代码。输出更具确定性和针对性。有助于生成语法正确的代码。</td></tr><tr><td style=text-align:left>创意写作</td><td style=text-align:left>0.7</td><td style=text-align:left>0.8</td><td>生成富有创意且多样化的叙事文本。输出更具探索性，且不受模式限制。</td></tr><tr><td style=text-align:left>聊天机器人回复</td><td style=text-align:left>0.5</td><td style=text-align:left>0.5</td><td>生成兼顾连贯性和多样性的对话回复。输出结果更加自然、引人入胜。</td></tr><tr><td style=text-align:left>代码注释生成</td><td style=text-align:left>0.3</td><td style=text-align:left>0.2</td><td>生成更简洁、更相关的代码注释。输出更具确定性，并符合规范。</td></tr><tr><td style=text-align:left>数据分析脚本</td><td style=text-align:left>0.2</td><td style=text-align:left>0.1</td><td>生成更准确、更高效的数据分析脚本。输出结果更具确定性和针对性。</td></tr></tbody></table><blockquote><p>表格参考自 <a href=https://community.openai.com/t/cheat-sheet-mastering-temperature-and-top-p-in-chatgpt-api/172683>Cheat Sheet: Mastering Temperature and Top-p in ChatGPT API</a></p></blockquote><h3 id=二者的执行顺序>二者的执行顺序<a hidden class=anchor aria-hidden=true href=#二者的执行顺序>#</a></h3><p>网络上很少关于 <code>temperature</code> 和 <code>top_p</code> 的执行顺序的资料。所以直接找到 Transformer 源码老家，看看具体实现。</p><p>定位到 <code>_get_logits_warper</code> 函数，这个函数负责处理 logits 的变换，这里按顺序往一个 warper（TFLogitsProcessorList 的实例）中插入了 <code>temperature</code>、<code>top_k</code> 和 <code>top_p</code> 的处理器（TF***LogitsWarper）。</p><p><img alt="Transformer source code _get_logits_warper" loading=lazy src=https://s2.loli.net/2025/07/23/R9PSZU8JuEh1Xmi.png></p><p>接着看一下 TFLogitsProcessorList 这个类的方法，发现只有 <code>__call__</code> 方法，也就是可以将该 warper 进行调用，具体逻辑是使用 for 循环按序调用每个处理器。</p><p><img alt="Transformer source code TFLogitsProcessorList" loading=lazy src=https://s2.loli.net/2025/07/23/YHlA5zFatR4yqej.png></p><p>所以，<code>temperature</code> 和 <code>top_p</code> 两个参数的应用顺序为：<strong>先应用 <code>temperature</code>，再应用 <code>top_p</code></strong>。</p><h3 id=总结>总结<a hidden class=anchor aria-hidden=true href=#总结>#</a></h3><p><code>temperature</code> 是一个强大而直观的工具，它让我们能够精细地控制 AI 的创造力。通过理解其背后的原理并学会在合适的场景下应用它，我们不仅能让 AI 的输出更符合我们的预期，还能解锁其在创意和探索方面的巨大潜力。下次当你与 AI 互动时，不妨尝试调整一下这个“温度”旋钮，看看会发生什么奇妙的变化。</p><h3 id=对业务的思考>对业务的思考<a hidden class=anchor aria-hidden=true href=#对业务的思考>#</a></h3><p>在实际业务中，作为 LLM 应用层开发者的我们不只有 All in <strong>Prompt Engineering</strong> 这条路来提升模型表现，也可以利用 temperature 这类采样参数进行精细化调控。像 LangChain、Eino 等框架，都提供了调整 temperature、top_p 等参数的能力（详见<a href=https://www.cloudwego.io/docs/eino/core_modules/components/chat_model_guide/#common-options>文档</a>）。
在构建多 Agent 系统时，我们要对每个 Agent 的职责进行明确划分，并对其采样参数进行<strong>量身定制</strong>，做到控制 Agent 的输出风格和内容质量。</p><h2 id=参考>参考<a hidden class=anchor aria-hidden=true href=#参考>#</a></h2><ul><li><a href=https://www.mdrk.io/temperature-samplig-in-ai/>https://www.mdrk.io/temperature-samplig-in-ai/</a></li><li><a href=https://community.openai.com/t/cheat-sheet-mastering-temperature-and-top-p-in-chatgpt-api/172683>https://community.openai.com/t/cheat-sheet-mastering-temperature-and-top-p-in-chatgpt-api/172683</a></li><li><a href="https://www.bilibili.com/video/BV1xmA2eMEFF/?spm_id_from=333.337.search-card.all.click&amp;vd_source=77e39221f40d20651d3556c53a9c7622">https://www.bilibili.com/video/BV1xmA2eMEFF/?spm_id_from=333.337.search-card.all.click&amp;vd_source=77e39221f40d20651d3556c53a9c7622</a></li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://blog.cassdev.com/tags/llm/>LLM</a></li><li><a href=https://blog.cassdev.com/tags/ai/>AI</a></li></ul></footer></article></main><footer class=footer><span>Copyright © 2025-2025 Cassius0924. All rights reserved.</span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="复制";function s(){t.innerHTML="已复制！",setTimeout(()=>{t.innerHTML="复制"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>